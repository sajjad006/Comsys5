{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12314612,"sourceType":"datasetVersion","datasetId":7762166}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:28:57.947310Z","iopub.execute_input":"2025-07-05T17:28:57.947553Z","iopub.status.idle":"2025-07-05T17:28:57.951768Z","shell.execute_reply.started":"2025-07-05T17:28:57.947535Z","shell.execute_reply":"2025-07-05T17:28:57.951053Z"},"id":"0u-zkIp5g8Ol"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport random\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass TripletFaceDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        root_dir/\n          └── <person_id>/\n               ├── img1.jpg\n               ├── img2.jpg\n               └── distortion/\n                    ├── img1_blurred.jpg\n                    ├── img1_foggy.jpg\n                    ├── img2_blurred.jpg\n                    └── …\n        \"\"\"\n        self.root = root_dir\n        self.transform = transform\n\n        # Build a list of (anchor_path, [positives], label)\n        self.samples = []\n        for person in sorted(os.listdir(root_dir)):\n            pdir = os.path.join(root_dir, person)\n            if not os.path.isdir(pdir):\n                continue\n\n            # 1) Gather all “clean” images in the person folder\n            clean_imgs = [\n                os.path.join(pdir, f)\n                for f in os.listdir(pdir)\n                if f.lower().endswith(('.jpg','.jpeg','.png'))\n                and os.path.isfile(os.path.join(pdir, f))\n            ]\n\n            # 2) Gather all distorted images\n            dist_dir = os.path.join(pdir, 'distortion')\n            if not os.path.isdir(dist_dir):\n                continue\n            distorted = glob.glob(os.path.join(dist_dir, '*.*'))\n\n            # 3) For each clean image, find its distortions by filename prefix\n            #    e.g. “Ciro_Gomes_0001.jpg” → distortions starting with “Ciro_Gomes_0001_”\n            for anchor_path in clean_imgs:\n                basename = os.path.splitext(os.path.basename(anchor_path))[0]\n                positives = [\n                    d for d in distorted\n                    if os.path.basename(d).startswith(basename + '_')\n                ]\n                if positives:\n                    self.samples.append((anchor_path, positives, person))\n\n        # keep list of all person labels for negative sampling\n        self.labels = sorted(set(label for _,_,label in self.samples))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        anchor_path, positives, label = self.samples[idx]\n        positive_path = random.choice(positives)\n\n        # Negative sampling: pick a *different* person\n        neg_label = random.choice([l for l in self.labels if l != label])\n        neg_idx = random.choice([\n            i for i,(_,_,lab) in enumerate(self.samples) if lab == neg_label\n        ])\n        neg_anchor, neg_positives, _ = self.samples[neg_idx]\n        negative_path = random.choice([neg_anchor] + neg_positives)\n\n        def load_img(p):\n            img = Image.open(p).convert('RGB')\n            return self.transform(img) if self.transform else img\n\n        return load_img(anchor_path), load_img(positive_path), load_img(negative_path)\n","metadata":{"id":"3l_cZBEzxmBu","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:29:25.432628Z","iopub.execute_input":"2025-07-05T17:29:25.433352Z","iopub.status.idle":"2025-07-05T17:29:25.442885Z","shell.execute_reply.started":"2025-07-05T17:29:25.433328Z","shell.execute_reply":"2025-07-05T17:29:25.442277Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class FaceEncoder(nn.Module):\n    def __init__(self, embedding_dim=128):\n        super().__init__()\n        resnet = models.resnet18(pretrained=True)\n        resnet.fc = nn.Identity()\n        self.backbone = resnet\n        self.embedding = nn.Linear(512, embedding_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.embedding(x)\n        x = F.normalize(x, p=2, dim=1)\n        return x\n","metadata":{"id":"mUslo8CAyu38","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:29:27.895651Z","iopub.execute_input":"2025-07-05T17:29:27.896255Z","iopub.status.idle":"2025-07-05T17:29:27.901141Z","shell.execute_reply.started":"2025-07-05T17:29:27.896230Z","shell.execute_reply":"2025-07-05T17:29:27.900415Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ntrain_dataset = TripletFaceDataset('/kaggle/input/comsys-hackathon/Task_B/train', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","metadata":{"id":"t0LagAkNyzIC","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:29:52.164888Z","iopub.execute_input":"2025-07-05T17:29:52.165197Z","iopub.status.idle":"2025-07-05T17:30:00.429910Z","shell.execute_reply.started":"2025-07-05T17:29:52.165173Z","shell.execute_reply":"2025-07-05T17:30:00.429362Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = FaceEncoder().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\ndef triplet_loss(anchor, positive, negative, margin=1.0):\n    pos_dist = F.pairwise_distance(anchor, positive)\n    neg_dist = F.pairwise_distance(anchor, negative)\n    return F.relu(pos_dist - neg_dist + margin).mean()\n\n# Training loop\nfor epoch in range(10):\n    model.train()\n    total_loss = 0\n    for anchor, positive, negative in tqdm(train_loader):\n        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n\n        anchor_emb = model(anchor)\n        pos_emb = model(positive)\n        neg_emb = model(negative)\n\n        loss = triplet_loss(anchor_emb, pos_emb, neg_emb)\n        total_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpr-tmD8zEBq","outputId":"426a4b80-418b-43b5-87f2-8265a8fc20e3","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:30:02.782493Z","iopub.execute_input":"2025-07-05T17:30:02.783150Z","iopub.status.idle":"2025-07-05T17:36:09.117768Z","shell.execute_reply.started":"2025-07-05T17:30:02.783120Z","shell.execute_reply":"2025-07-05T17:36:09.116787Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 207MB/s]\n100%|██████████| 121/121 [00:41<00:00,  2.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.2486\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:38<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.0746\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:34<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.0492\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:37<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.0444\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:34<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.0337\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:35<00:00,  3.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 0.0300\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:35<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 0.0249\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:35<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 0.0196\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:37<00:00,  3.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 0.0203\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121/121 [00:35<00:00,  3.45it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 0.0163\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def get_embeddings(model, dataset_dir):\n    model.eval()\n    embeddings = {}\n    with torch.no_grad():\n        for person in os.listdir(dataset_dir):\n            p_dir = os.path.join(dataset_dir, person)\n            img_path = os.path.join(p_dir, '001_frontal.jpg')\n            if not os.path.exists(img_path): continue\n            img = Image.open(img_path).convert('RGB')\n            img = transform(img).unsqueeze(0).to(device)\n            emb = model(img).cpu().squeeze(0)\n            embeddings[person] = emb\n    return embeddings\n","metadata":{"id":"1i4O3BFq3CZ-","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:36:47.526965Z","iopub.execute_input":"2025-07-05T17:36:47.527276Z","iopub.status.idle":"2025-07-05T17:36:47.532749Z","shell.execute_reply.started":"2025-07-05T17:36:47.527249Z","shell.execute_reply":"2025-07-05T17:36:47.531948Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"torch.save(model.state_dict(), 'triplet_face_model.pth')\n","metadata":{"id":"Fu2K4t883GEM","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:36:51.326778Z","iopub.execute_input":"2025-07-05T17:36:51.327027Z","iopub.status.idle":"2025-07-05T17:36:51.406020Z","shell.execute_reply.started":"2025-07-05T17:36:51.327008Z","shell.execute_reply":"2025-07-05T17:36:51.405257Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os, glob, torch, numpy as np\nimport torch.nn.functional as F\nfrom PIL import Image\n","metadata":{"id":"Lj7ypkgl3uL9","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:36:54.338746Z","iopub.execute_input":"2025-07-05T17:36:54.339642Z","iopub.status.idle":"2025-07-05T17:36:54.343191Z","shell.execute_reply.started":"2025-07-05T17:36:54.339618Z","shell.execute_reply":"2025-07-05T17:36:54.342549Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def build_gallery(model, val_root, transform, device):\n    \"\"\"\n    Returns: dict person_id -> embedding tensor (D,)\n    \"\"\"\n    model.eval()\n    gallery = {}\n    with torch.no_grad():\n        for person in sorted(os.listdir(val_root)):\n            pdir = os.path.join(val_root, person)\n            # find any “clean” image at person/\n            clean_imgs = [\n                os.path.join(pdir, f) for f in os.listdir(pdir)\n                if f.lower().endswith(('.jpg','png','jpeg'))\n                   and not f.startswith('distortion')\n            ]\n            if not clean_imgs:\n                continue\n            # pick the first clean image (or average multiple if you prefer)\n            img = Image.open(clean_imgs[0]).convert('RGB')\n            x = transform(img).unsqueeze(0).to(device)\n            emb = model(x).cpu().squeeze(0)  # (D,)\n            gallery[person] = emb\n    return gallery","metadata":{"id":"5NefGeID3zQE","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:36:58.905832Z","iopub.execute_input":"2025-07-05T17:36:58.906119Z","iopub.status.idle":"2025-07-05T17:36:58.911724Z","shell.execute_reply.started":"2025-07-05T17:36:58.906096Z","shell.execute_reply":"2025-07-05T17:36:58.911101Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.load_state_dict(torch.load('triplet_face_model.pth', map_location=device))\nmodel.to(device)\ngallery = build_gallery(model, '/kaggle/input/comsys-hackathon/Task_B/val', transform, device)\n","metadata":{"id":"S8ie5BgL32pD","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:37:15.939970Z","iopub.execute_input":"2025-07-05T17:37:15.940615Z","iopub.status.idle":"2025-07-05T17:37:22.086616Z","shell.execute_reply.started":"2025-07-05T17:37:15.940589Z","shell.execute_reply":"2025-07-05T17:37:22.086015Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def evaluate(model, gallery, val_root, transform, device):\n    model.eval()\n    total, correct = 0, 0\n\n    # for per‑distortion stats\n    per_type = {}    # e.g. { 'blurred': [correct, total], … }\n\n    with torch.no_grad():\n        for person, emb_g in gallery.items():\n            pdir = os.path.join(val_root, person, 'distortion')\n            for dist_path in glob.glob(os.path.join(pdir, '*.*')):\n\n                # 1) True label\n                true_id = person\n\n                # 2) Distortion type from filename\n                #    e.g. “Ciro_Gomes_0001_blurred.jpg” → “blurred”\n                dtype = os.path.splitext(dist_path)[0].split('_')[-1]\n                per_type.setdefault(dtype, [0,0])\n\n                # 3) Compute embedding\n                img = Image.open(dist_path).convert('RGB')\n                x = transform(img).unsqueeze(0).to(device)\n                q_emb = model(x).cpu().squeeze(0)\n\n                # 4) Cosine similarities vs gallery\n                sims = {pid: F.cosine_similarity(q_emb, g_emb, dim=0).item()\n                        for pid, g_emb in gallery.items()}\n\n                # 5) Find best match\n                pred = max(sims, key=sims.get)\n\n                # 6) Tally\n                total += 1\n                per_type[dtype][1] += 1\n                if pred == true_id:\n                    correct += 1\n                    per_type[dtype][0] += 1\n\n    overall_acc = correct / total\n    print(f\"🟢 Overall Top‑1 Accuracy: {overall_acc*100:.2f}%  ({correct}/{total})\")\n\n    print(\"\\n🔍 Accuracy by Distortion Type:\")\n    for dtype, (c,t) in per_type.items():\n        print(f\"  • {dtype:10s}: { (c/t*100) if t>0 else 0:6.2f}% ({c}/{t})\")\n\n    return overall_acc, per_type\n\n# usage\nevaluate(model, gallery, '/content/Comys_Hackathon5/Task_B/val', transform, device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8dHouuK4AFb","outputId":"567ea901-11ce-46c7-cb00-46ebdb021dfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["🟢 Overall Top‑1 Accuracy: 61.31%  (1811/2954)\n","\n","🔍 Accuracy by Distortion Type:\n","  • resized   :  61.14% (258/422)\n","  • blurred   :  61.85% (261/422)\n","  • lowlight  :  61.37% (259/422)\n","  • sunny     :  60.90% (257/422)\n","  • noisy     :  61.14% (258/422)\n","  • foggy     :  61.37% (259/422)\n","  • rainy     :  61.37% (259/422)\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6130670277589709,\n"," {'resized': [258, 422],\n","  'blurred': [261, 422],\n","  'lowlight': [259, 422],\n","  'sunny': [257, 422],\n","  'noisy': [258, 422],\n","  'foggy': [259, 422],\n","  'rainy': [259, 422]})"]},"metadata":{},"execution_count":27}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ArcMarginProduct(nn.Module):\n    \"\"\"\n    Implements the ArcFace angular margin penalty.\n    \"\"\"\n    def __init__(self, embedding_size, num_classes, s=30.0, m=0.50, easy_margin=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.embedding_size = embedding_size\n        self.s = s      # norm scale\n        self.m = m      # angular margin\n        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_size))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n\n    def forward(self, embeddings, labels):\n        # Normalize features and weights\n        cosine = F.linear(F.normalize(embeddings), F.normalize(self.weight))  # [B, C]\n        sine = torch.sqrt(1.0 - cosine.pow(2))\n        phi = cosine * self.cos_m - sine * self.sin_m   # cos(θ + m)\n\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n\n        # convert labels to one-hot\n        one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(embeddings.device)\n\n        # margin apply: where one_hot==1 use phi, else use cosine\n        logits = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        logits *= self.s\n\n        return logits\n","metadata":{"id":"kLdwVCYk5qy_","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:37:40.400891Z","iopub.execute_input":"2025-07-05T17:37:40.401730Z","iopub.status.idle":"2025-07-05T17:37:40.409260Z","shell.execute_reply.started":"2025-07-05T17:37:40.401703Z","shell.execute_reply":"2025-07-05T17:37:40.408528Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class FaceModelWithArcFace(nn.Module):\n    def __init__(self, num_classes, embedding_size=128):\n        super().__init__()\n        # backbone\n        resnet = models.resnet18(pretrained=True)\n        resnet.fc = nn.Identity()\n        self.backbone = resnet\n\n        # embedding layer\n        self.embedding = nn.Linear(512, embedding_size)\n\n        # ArcFace head\n        self.arc_margin = ArcMarginProduct(\n            embedding_size, num_classes, s=30.0, m=0.50, easy_margin=False\n        )\n\n    def forward(self, x, labels):\n        x = self.backbone(x)                 # [B,512]\n        emb = F.normalize(self.embedding(x)) # [B,128]\n        logits = self.arc_margin(emb, labels)\n        return logits, emb\n","metadata":{"id":"q9iR3Mgp5uBH","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:37:44.380816Z","iopub.execute_input":"2025-07-05T17:37:44.381112Z","iopub.status.idle":"2025-07-05T17:37:44.386341Z","shell.execute_reply.started":"2025-07-05T17:37:44.381060Z","shell.execute_reply":"2025-07-05T17:37:44.385627Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass FaceClassificationDataset(Dataset):\n    \"\"\"\n    Scans:\n      root_dir/\n        ├── personA/\n        │     ├── img1.jpg\n        │     ├── img2.jpg\n        │     └── distortion/\n        │           ├── img1_blurred.jpg\n        │           └── …\n        └── personB/\n              └── …\n    And returns (image_tensor, label_idx).\n    \"\"\"\n    def __init__(self, root_dir, transform=None):\n        self.transform = transform\n\n        # build mapping person_id -> class_idx\n        self.persons = sorted(d for d in os.listdir(root_dir)\n                              if os.path.isdir(os.path.join(root_dir, d)))\n        self.label2idx = {p:i for i,p in enumerate(self.persons)}\n\n        # gather (image_path, label_idx) for both clean + distorted\n        self.samples = []\n        for person in self.persons:\n            pdir = os.path.join(root_dir, person)\n\n            # 1) clean images in pdir\n            for fn in os.listdir(pdir):\n                if fn.lower().endswith(('.jpg','.jpeg','.png')):\n                    self.samples.append((\n                        os.path.join(pdir, fn),\n                        self.label2idx[person]\n                    ))\n\n            # 2) distortions\n            dist_dir = os.path.join(pdir, 'distortion')\n            if os.path.isdir(dist_dir):\n                for fn in os.listdir(dist_dir):\n                    if fn.lower().endswith(('.jpg','.jpeg','.png')):\n                        self.samples.append((\n                            os.path.join(dist_dir, fn),\n                            self.label2idx[person]\n                        ))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n","metadata":{"id":"Jgepag1q73or","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:37:56.969050Z","iopub.execute_input":"2025-07-05T17:37:56.969337Z","iopub.status.idle":"2025-07-05T17:37:56.976808Z","shell.execute_reply.started":"2025-07-05T17:37:56.969318Z","shell.execute_reply":"2025-07-05T17:37:56.976113Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])\n\ntrain_cls_ds = FaceClassificationDataset('/kaggle/input/comsys-hackathon/Task_B/train', transform)\ntrain_loader = DataLoader(\n    train_cls_ds,\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiQ8ZfYn7763","outputId":"dd581b98-f689-47b4-9ecf-1129d4f18bec","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:38:06.116834Z","iopub.execute_input":"2025-07-05T17:38:06.117499Z","iopub.status.idle":"2025-07-05T17:38:08.351519Z","shell.execute_reply.started":"2025-07-05T17:38:06.117474Z","shell.execute_reply":"2025-07-05T17:38:08.350716Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = FaceModelWithArcFace(\n    num_classes=len(train_cls_ds.persons),\n    embedding_size=128\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(1, 11):\n    model.train()\n    total_loss = 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        logits, embeddings = model(imgs, labels)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch:02d} — Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1rOnU565xDV","outputId":"d4c8ec3d-6b7e-432b-d2c7-e8220041cfed","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:38:11.546411Z","iopub.execute_input":"2025-07-05T17:38:11.546677Z","iopub.status.idle":"2025-07-05T17:48:13.488200Z","shell.execute_reply.started":"2025-07-05T17:38:11.546657Z","shell.execute_reply":"2025-07-05T17:48:13.487322Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 — Loss: 16.0488\nEpoch 02 — Loss: 4.2328\nEpoch 03 — Loss: 0.4665\nEpoch 04 — Loss: 0.0556\nEpoch 05 — Loss: 0.0137\nEpoch 06 — Loss: 0.0098\nEpoch 07 — Loss: 0.0073\nEpoch 08 — Loss: 0.0056\nEpoch 09 — Loss: 0.0045\nEpoch 10 — Loss: 0.0038\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"torch.save(model.state_dict(), 'arcface_model.pth')","metadata":{"id":"pqBp_VRpAV2j","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:48:21.098943Z","iopub.execute_input":"2025-07-05T17:48:21.099754Z","iopub.status.idle":"2025-07-05T17:48:21.176483Z","shell.execute_reply.started":"2025-07-05T17:48:21.099721Z","shell.execute_reply":"2025-07-05T17:48:21.175905Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# instantiate the ArcFace model architecture\nmodel = FaceModelWithArcFace(num_classes=877, embedding_size=128).to(device)\n# now load the correct weights\nmodel.load_state_dict(torch.load('arcface_model.pth', map_location=device))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9T3NLTNsAiCq","outputId":"6d96dbf8-fde9-4df7-f047-d1e074d0e429","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:48:24.244935Z","iopub.execute_input":"2025-07-05T17:48:24.245288Z","iopub.status.idle":"2025-07-05T17:48:24.527233Z","shell.execute_reply.started":"2025-07-05T17:48:24.245264Z","shell.execute_reply":"2025-07-05T17:48:24.526513Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision import transforms\n# from your_model_file import FaceModelWithArcFace  # adjust import to where you defined the class\n\n# ——————————————————————————————————————————\n# 1) Config & Device\n# ——————————————————————————————————————————\ndevice    = 'cuda' if torch.cuda.is_available() else 'cpu'\nval_root  = '/kaggle/input/comsys-hackathon/Task_B/val'         # path to your validation folder\nmodel_path = 'arcface_model.pth'          # your ArcFace‐trained weights\n\n# ——————————————————————————————————————————\n# 2) Data Transform (must match training)\n# ——————————————————————————————————————————\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5],\n                         [0.5, 0.5, 0.5]),\n])\n\n# ——————————————————————————————————————————\n# 3) Load ArcFace Model\n# ——————————————————————————————————————————\nnum_classes    = 877   # update if different\nembedding_size = 128   # same as during training\n\nmodel = FaceModelWithArcFace(num_classes, embedding_size).to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# ——————————————————————————————————————————\n# 4) Build Validation Gallery\n# ——————————————————————————————————————————\ndef build_gallery(model, val_root, transform, device):\n    \"\"\"\n    Returns: dict person_id -> L2‐normalized averaged embedding (tensor, shape [D])\n    \"\"\"\n    gallery = {}\n    with torch.no_grad():\n        for person in sorted(os.listdir(val_root)):\n            pdir = os.path.join(val_root, person)\n            if not os.path.isdir(pdir):\n                continue\n\n            # collect all “clean” images (ignore subfolders)\n            clean_imgs = [\n                os.path.join(pdir, f)\n                for f in os.listdir(pdir)\n                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n                and os.path.isfile(os.path.join(pdir, f))\n            ]\n            if not clean_imgs:\n                continue\n\n            embs = []\n            for img_path in clean_imgs:\n                img = Image.open(img_path).convert('RGB')\n                x = transform(img).unsqueeze(0).to(device)\n                # forward with dummy labels for embedding\n                _, emb = model(x, torch.zeros(1, dtype=torch.long, device=device))\n                embs.append(emb.cpu())\n\n            # average and re‐normalize\n            emb_avg = torch.stack(embs, dim=0).mean(dim=0)\n            gallery[person] = F.normalize(emb_avg, p=2, dim=1).squeeze(0)\n\n    return gallery\n\n# ——————————————————————————————————————————\n# 5) Evaluate on Distortions\n# ——————————————————————————————————————————\ndef evaluate(model, gallery, val_root, transform, device):\n    total = correct = 0\n    per_type = {}\n\n    with torch.no_grad():\n        for person, g_emb in gallery.items():\n            dist_dir = os.path.join(val_root, person, 'distortion')\n            if not os.path.isdir(dist_dir):\n                continue\n\n            for dist_path in glob.glob(os.path.join(dist_dir, '*.*')):\n                # true ID\n                true_id = person\n                # extract distortion type from filename suffix\n                dtype = os.path.splitext(os.path.basename(dist_path))[0].split('_')[-1]\n                per_type.setdefault(dtype, [0, 0])\n\n                # compute query embedding\n                img = Image.open(dist_path).convert('RGB')\n                x = transform(img).unsqueeze(0).to(device)\n                _, q_emb = model(x, torch.zeros(1, dtype=torch.long, device=device))\n                q_emb = q_emb.cpu()\n\n                # cosine similarity vs all gallery embeddings\n                sims = {pid: F.cosine_similarity(q_emb, emb.unsqueeze(0), dim=1).item()\n                        for pid, emb in gallery.items()}\n                pred = max(sims, key=sims.get)\n\n                # tally results\n                total += 1\n                per_type[dtype][1] += 1\n                if pred == true_id:\n                    correct += 1\n                    per_type[dtype][0] += 1\n\n    overall_acc = correct / total * 100\n    print(f\"\\n🟢 Overall Top‑1 Accuracy: {overall_acc:.2f}% ({correct}/{total})\\n\")\n    print(\"🔍 Accuracy by Distortion Type:\")\n    for dtype, (c, t) in per_type.items():\n        acc = (c / t * 100) if t > 0 else 0\n        print(f\"  • {dtype:10s}: {acc:6.2f}% ({c}/{t})\")\n\n    return overall_acc, per_type\n\n# ——————————————————————————————————————————\n# 6) Run Evaluation\n# ——————————————————————————————————————————\ngallery = build_gallery(model, val_root, transform, device)\nevaluate(model, gallery, val_root, transform, device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msgClULoAlUg","outputId":"174fdb62-adb1-4551-9a15-40e839220c54","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:48:43.355819Z","iopub.execute_input":"2025-07-05T17:48:43.356569Z","iopub.status.idle":"2025-07-05T17:49:54.877371Z","shell.execute_reply.started":"2025-07-05T17:48:43.356535Z","shell.execute_reply":"2025-07-05T17:49:54.876815Z"}},"outputs":[{"name":"stdout","text":"\n🟢 Overall Top‑1 Accuracy: 93.84% (2772/2954)\n\n🔍 Accuracy by Distortion Type:\n  • foggy     :  97.39% (411/422)\n  • lowlight  :  96.92% (409/422)\n  • noisy     :  89.57% (378/422)\n  • blurred   :  95.02% (401/422)\n  • rainy     :  93.84% (396/422)\n  • sunny     :  87.20% (368/422)\n  • resized   :  96.92% (409/422)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(93.8388625592417,\n {'foggy': [411, 422],\n  'lowlight': [409, 422],\n  'noisy': [378, 422],\n  'blurred': [401, 422],\n  'rainy': [396, 422],\n  'sunny': [368, 422],\n  'resized': [409, 422]})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate_with_metrics(model, gallery, val_root, transform, device):\n    total = correct = 0\n    per_type = {}\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for person, g_emb in gallery.items():\n            dist_dir = os.path.join(val_root, person, 'distortion')\n            if not os.path.isdir(dist_dir):\n                continue\n\n            for dist_path in glob.glob(os.path.join(dist_dir, '*.*')):\n                true_id = person\n                dtype = os.path.splitext(os.path.basename(dist_path))[0].split('_')[-1]\n                per_type.setdefault(dtype, [0, 0])\n\n                img = Image.open(dist_path).convert('RGB')\n                x = transform(img).unsqueeze(0).to(device)\n                _, q_emb = model(x, torch.zeros(1, dtype=torch.long, device=device))\n                q_emb = q_emb.cpu()\n\n                sims = {pid: F.cosine_similarity(q_emb, emb.unsqueeze(0), dim=1).item()\n                        for pid, emb in gallery.items()}\n                pred = max(sims, key=sims.get)\n\n                y_true.append(true_id)\n                y_pred.append(pred)\n\n                total += 1\n                per_type[dtype][1] += 1\n                if pred == true_id:\n                    correct += 1\n                    per_type[dtype][0] += 1\n\n    # Metrics\n    acc = accuracy_score(y_true, y_pred) * 100\n    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\n    print(f\"\\n🟢 Overall Accuracy: {acc:.2f}%\")\n    print(f\"📊 Precision: {prec:.4f}\")\n    print(f\"📈 Recall:    {rec:.4f}\")\n    print(f\"🏅 F1 Score:  {f1:.4f}\")\n    print(\"\\n🔍 Accuracy by Distortion Type:\")\n    for dtype, (c, t) in per_type.items():\n        acc_d = (c / t * 100) if t > 0 else 0\n        print(f\"  • {dtype:10s}: {acc_d:6.2f}% ({c}/{t})\")\n\n    return {\n        \"accuracy\": acc,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1_score\": f1,\n        \"per_type\": per_type\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:59:48.211818Z","iopub.execute_input":"2025-07-05T17:59:48.212226Z","iopub.status.idle":"2025-07-05T17:59:48.928974Z","shell.execute_reply.started":"2025-07-05T17:59:48.212201Z","shell.execute_reply":"2025-07-05T17:59:48.928419Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Validation\nval_metrics = evaluate_with_metrics(model, gallery, val_root, transform, device)\n\n# Training (if needed)\ntrain_root = '/kaggle/input/comsys-hackathon/Task_B/train'\ntrain_gallery = build_gallery(model, train_root, transform, device)\ntrain_metrics = evaluate_with_metrics(model, train_gallery, train_root, transform, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T17:59:58.735529Z","iopub.execute_input":"2025-07-05T17:59:58.736176Z","iopub.status.idle":"2025-07-05T18:08:46.458412Z","shell.execute_reply.started":"2025-07-05T17:59:58.736153Z","shell.execute_reply":"2025-07-05T18:08:46.457704Z"}},"outputs":[{"name":"stdout","text":"\n🟢 Overall Accuracy: 93.84%\n📊 Precision: 0.9353\n📈 Recall:    0.9864\n🏅 F1 Score:  0.9559\n\n🔍 Accuracy by Distortion Type:\n  • foggy     :  97.39% (411/422)\n  • lowlight  :  96.92% (409/422)\n  • noisy     :  89.57% (378/422)\n  • blurred   :  95.02% (401/422)\n  • rainy     :  93.84% (396/422)\n  • sunny     :  87.20% (368/422)\n  • resized   :  96.92% (409/422)\n\n🟢 Overall Accuracy: 100.00%\n📊 Precision: 1.0000\n📈 Recall:    1.0000\n🏅 F1 Score:  1.0000\n\n🔍 Accuracy by Distortion Type:\n  • blurred   : 100.00% (1926/1926)\n  • resized   : 100.00% (1926/1926)\n  • noisy     : 100.00% (1926/1926)\n  • foggy     : 100.00% (1926/1926)\n  • sunny     : 100.00% (1926/1926)\n  • rainy     : 100.00% (1926/1926)\n  • lowlight  : 100.00% (1926/1926)\n","output_type":"stream"}],"execution_count":21}]}